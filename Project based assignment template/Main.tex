\documentclass[a4paper, twoside]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage[T2A]{fontenc}

%Hyphenation rules
%--------------------------------------
\usepackage{hyphenat}
\hyphenation{ма-те-ма-ти-ка вос-ста-нав-ли-вать}
%--------------------------------------
\usepackage[english, russian]{babel}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage[top=3cm, bottom=4cm, left=3.5cm, right=3.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd, fancyhdr, color, comment, graphicx, environ}
\usepackage{float}
\usepackage{mathrsfs}
\usepackage[math-style=ISO]{unicode-math}
%\setmathfont{TeX Gyre Termes Math}
\usepackage{lastpage}
\usepackage[dvipsnames]{xcolor}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{fancyhdr}
\usepackage{indentfirst}
\usepackage{listings}
\usepackage{sectsty}
\usepackage{thmtools}
\usepackage{shadethm}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{longtable}
\usepackage{pdfpages}
\usepackage{listings}
\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      

}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Environment setup
\mdfsetup{skipabove=\topskip,skipbelow=\topskip}
\newrobustcmd\ExampleText{%
An \textit{inhomogeneous linear} differential equation has the form
\begin{align}
L[v ] = f,
\end{align}
where $L$ is a linear differential operator, $v$ is the dependent
variable, and $f$ is a given non−zero function of the independent
variables alone.
}
\mdfdefinestyle{theoremstyle}{%
linecolor=black,linewidth=1pt,%
frametitlerule=true,%
frametitlebackgroundcolor=gray!20,
innertopmargin=\topskip,
}
\mdtheorem[style=theoremstyle]{Problem}{Problem}
\newenvironment{Solution}{\textbf{Solution.}}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Fill in the appropriate information below
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}     
\newcommand\course{XXXX0000}                            % <-- course name   
\newcommand\hwnumber{0}                                 % <-- homework number
\newcommand\Information{Someone}                        % <-- personal information
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Page setup
\pagestyle{fancy}
\headheight 35pt
\lhead{\today}
\rhead{\includegraphics[width=2.5cm]{latex-und-logo.png}}
\lfoot{}
\pagenumbering{arabic}
\cfoot{\small\thepage}
\rfoot{}
\headsep 1.2em
\renewcommand{\baselinestretch}{1.25}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Add new commands here
\renewcommand{\labelenumi}{\alph{enumi})}
\newcommand{\Z}{\mathbb Z}
\newcommand{\R}{\mathbb R}
\newcommand{\Q}{\mathbb Q}
\newcommand{\NN}{\mathbb N}
\newcommand{\PP}{\mathbb P}
\DeclareMathOperator{\Mod}{Mod} 
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}
\newtheorem*{theorem}{Theorem}
\newtheorem*{lemma}{Lemma}
\newtheorem{case}{Case}
\newcommand{\assign}{:=}
\newcommand{\infixiff}{\text{ iff }}
\newcommand{\nobracket}{}
\newcommand{\backassign}{=:}
\newcommand{\tmmathbf}[1]{\ensuremath{\boldsymbol{#1}}}
\newcommand{\tmop}[1]{\ensuremath{\operatorname{#1}}}
\newcommand{\tmtextbf}[1]{\text{{\bfseries{#1}}}}
\newcommand{\tmtextit}[1]{\text{{\itshape{#1}}}}

\newenvironment{itemizedot}{\begin{itemize} \renewcommand{\labelitemi}{$\bullet$}\renewcommand{\labelitemii}{$\bullet$}\renewcommand{\labelitemiii}{$\bullet$}\renewcommand{\labelitemiv}{$\bullet$}}{\end{itemize}}
\catcode`\<=\active \def<{
\fontencoding{T1}\selectfont\symbol{60}\fontencoding{\encodingdefault}}
\catcode`\>=\active \def>{
\fontencoding{T1}\selectfont\symbol{62}\fontencoding{\encodingdefault}}
\catcode`\<=\active \def<{
\fontencoding{T1}\selectfont\symbol{60}\fontencoding{\encodingdefault}}
\title{Title}
\author{Your Name}


\begin{document}
\input{title/title.tex}
\pagestyle{fancy}
\headheight 35pt
\lhead{\today}
\rhead{\includegraphics[width=2.5cm]{latex-und-logo.png}}
\lfoot{}
\pagenumbering{arabic}
\cfoot{\small\thepage}
\rfoot{}
\headsep 1.2em
\renewcommand{\baselinestretch}{1.25}\newpage 

\section{Numerical Solution of the Laplace’s Equation Using
Successive-over-relaxation (SOR) Method}
The 2D Laplace’s equation is, \\
\begin{center}
    
$\frac{\partial^2T}{\partial x^2} + \frac{\partial^2T}{\partial y^2} = 0, 0\leq x,y \leq 1 \rightarrow \textbf{Eqn. 1}$  \\
\end{center}

where T = T (x, y). Suppose Eqn. 1 is subject to the following boundary conditions: \\

\begin{center}
    T(x,0) = x $\rightarrow$ \textbf{Eqn. 2a} \\
    T(0,y) = y $\rightarrow$ \textbf{Eqn. 2b}\\
    T(1,y) = 1 $\rightarrow$ \textbf{Eqn. 2c}\\
    T(x,1) = 1 $\rightarrow$ \textbf{Eqn. 2d}
\end{center}

\textbf{Assumptions} 
\begin{center}
\begin{itemize}
    \item Assume there are Nx and Ny number of space divisions in x and y directions, respectively. 
    \item The number of discrete points in x and y directions are Nx + 1 and Ny + 1 respectively. 
    \item For this project, assume we are using the same number of discrete points in x and y directions, Nx = Ny. \\ 
    \item Length between adjacent discrete points: $\Delta x = \frac{1}{Nx}$, and $\Delta y = \frac{1}{Ny}$. \\
    \item $T^{(k)}_{i,j}$ is the numerical solution at iteration k corresponding to a discrete point indexed as (i, j) in the 2D mesh, or at coordinates ($x_i$, $y_j$ ) with $x_i = i\Delta x$ and $y_j = j\Delta y$.
\end{center}
\end{itemize}

\textbf{General Steps for SOR method:}
\begin{center}
\begin{enumerate}
    \item Start with an initial guess T (0) i,j of the numerical solution. For the initial guess, we can simply use: \\

     $T^{(0)}_{i,j}$ = 0, i = 0, 1, ..., Nx and j = 0, 1, ..., Ny. $\rightarrow$ \textbf{Eqn. 3}
    \item Use the point Gauss-Seidel SOR method to find the iterative solution, $T^{(k+1)}_{i,j}$ , assuming $T^{(k)}_{i,j}$ is known and k is the iteration number with k $\geq$ 0.
    \item To ensure the solution of the iterative method is converged, we monitor the solution
    difference between successive iterations as $\delta^{(k)}$: \\
    $\delta^{(k)}$ = $max_{i,j}$ |$T^{(k+1)}_{i,j} - T^{(k)}_{i,j}$ | (4)
    \item We set a tolerance of $\epsilon_0=10^{-4}$. The iterative solution is converged if: \\
    $\frac{\delta^{(k)}}{\delta^{(0)}} \leq \epsilon_0 \rightarrow$ \textbf{Eqn. 5}
    \item If the solution is converged at iteration $k = k_0$, we can then stop the iteration and the
    final numerical solution will be $T_{i,j}$ = $T^{(k_0)}_{i,j}$ . Otherwise repeat Step 2-4 until reaching the convergence criterion, Eqn. 5.
\end{enumerate}
\end{center}
\subsection{2D counter plot of the final converged numerical solution}
Below is the 2D counter plot of the final converged numerical solution for when Nx=Ny=10. This counter graph makes sense because when examining the potential($\phi$) values and the given boundary conditions, we see that the top left corner, the top right corner, and the bottom right corner all are equal to 1. This complies with given boundary conditions. Furthermore, from heat transfer we see that heat dissipates over the spatial dimension. \textbf{Convergence reached after 44 iterations with omega=1.8}  \\
\begin{center}
    \includegraphics[width=15.00cm]{output_0_1.png} \\
    \textit{Figure 1 - 2D counter plot of the final converged numerical solution for when Nx=Ny=10}
\end{center}

\newpage
\subsection{x-y plot to show the numerical solution at y=0.50 when Nx=Ny=10}
Below is an x-y plot to show the convergence history of the iterative method when Nx=Ny=10. The plot makes sense because when examining figure 1 at y=0.50 and drawing a line directly right, we see that each value is increased by roughly 0.50 every iteration.  \\

\begin{center}
    \includegraphics[width=15.00cm]{output_0_2.png} \\
    \textit{Figure 2 - Numerical solution for when Nx=Ny=10}
\end{center}

\newpage
\subsection{x-y plot to show the convergence history of the iterative method when Nx=Ny=10}
Below is a plot of an x-y plot to show the convergence history of the iterative method when Nx=Ny=10. This figure makes sense because for when Nx=10, Ny=10, the last value in the convergence history is: \textbf{8.534642242818968e-05}. This meets the requirements of: \\

$\frac{\delta^{(k)}}{\delta^{(0)}} \leq \epsilon_0 $ \\

Furthermore, since the last value in the convergence history is less than $10^{-4}$, the numerical solution has converged and therefore stops. The 44 iterations is a good low number and is because low lengths are between adjacent discrete points: $\Delta x = \frac{1}{Nx}$, and $\Delta y = \frac{1}{Ny}$. 

\begin{center}
    \includegraphics[width=15.00cm]{output_0_3.png} \\
    \textit{Figure 3 - convergence history for when Nx=Ny=10}
\end{center}

\newpage
\section{Gauss-Seidel SOR method with relaxation parameter $\omega$ = 1.8 to
compute the solution for Nx = Ny = 50}
This question is a repeat of question one, but instead now Nx=Ny=50.

\subsection{2D counter plot of the final converged numerical solution}
Below is the 2D counter plot of the final converged numerical solution for when Nx=Ny=50. This counter graph makes sense because when examining the potential($\phi$) values and the given boundary conditions, we see that the top left corner, the top right corner, and the bottom right corner all are equal to 1. This complies with given boundary conditions. Furthermore, from heat transfer we see that heat dissipates over the spatial dimension. \textbf{Convergence reached after 171 iterations with omega=1.8} \\

\begin{center}
    \includegraphics[width=15.00cm]{output_0_5.png} \\
    \textit{Figure 4 - 2D counter plot of the final converged numerical solution for when Nx=Ny=50}
\end{center}

\newpage 
\subsection{x-y plot to show the numerical solution at y=0.50 when Nx=Ny=50}
Below is an x-y plot to show the convergence history of the iterative method when Nx=Ny=50. The plot makes sense because when examining figure 1 at y=0.50 and drawing a line directly right, we see that each value is increased by roughly 0.50 every iteration.  \\

\begin{center}
    \includegraphics[width=15.00cm]{output_0_6.png} \\
    \textit{Figure 5 - Numerical solution for when Nx=Ny=50}
\end{center}

\newpage 
\subsection{x-y plot to show the convergence history of the iterative method}
Below is a plot of an x-y plot to show the convergence history of the iterative method when Nx=Ny=10. This figure makes sense because for when Nx=50, Ny=50, the last value in the convergence history is: \textbf{9.954540001710921e-05}. This meets the requirements of: \\

$\frac{\delta^{(k)}}{\delta^{(0)}} \leq \epsilon_0 $ \\

Furthermore, since the last value in the convergence history is less than $10^{-4}$, the numerical solution has converged and therefore stops. The iterations in this question (171) is higher than in question one (44) because the lengths between adjacent discrete points are higher, therefore producing a larger value of iterations: $\Delta x = \frac{1}{Nx}$, and $\Delta y = \frac{1}{Ny}$. This plot is more smooth and/or linear than figure 3 because it took more iterations to achieve convergence, therefore provided more dicrete points on the plot. 

\begin{center}
    \includegraphics[width=15.00cm]{output_0_7.png} \\
    \textit{Figure 6 - convergence history for when Nx=Ny=50}
\end{center}

\newpage 
\section{Use the point Gauss-Seidel SOR method to find the optimal relaxation parameter value $\omega_o$ for Nx = Ny = 10}
This question was tasked with varying the relaxation parameter $\omega$ between 1.0 $\leq \omega \leq $ 1.95 with a step size of 0.05. So you will have 20 different values of relaxation parameter $\omega$, with $\omega$ = 1.0, 1.05, 1.1, 1.15, ..., 1.95. For each value of relaxation parameter $\omega$, find the number of iterations required for convergence $k_0$ according to the convergence criterion we set in Eqn. 5. Lastly, the number of iterations required for convergence, $k_0$, depends on $\omega$. $k_0 = k_0(\omega)$. In Appendix section 5.1, reference lines 91-119 in my python script for clarity on how the following plot and optimal value of the relaxation parameter were determined.

\subsection{x-y plot of the number of iterations required for convergence $k_o vs. \omega$}
The shape of the plot showing the relationship between the number of iterations required for convergence and the relaxation parameter, $k_o vs. \omega$, is influenced by several factors. When the grid size is small, such as when Nx=Ny=10, the optimal value for $\omega$ may be easier to identify because there are fewer interactions to consider within the grid. As a result, the U-shaped pattern typically associated with this plot may be more pronounced, with fewer iterations needed to reach convergence. Additionally, boundary conditions and convergence tolerance play a significant role in shaping the curve's behavior. Specific boundary conditions, along with the tolerance level for determining convergence, can impact the stability of the solution process. In smaller grids, these factors might lead to quicker stabilization, which could explain why the U-shape appears.
\begin{center}
    \includegraphics[width=15.00cm]{output_0_9.png} \\
    \textit{Figure 7 - number of iterations required for convergence $k_o vs. \omega$ when Nx=Ny=10 }
\end{center}

\newpage
\subsection{For Nx = Ny = 10, report the optimal value of the relaxation parameter, $\omega_o$}
Below is the optimal value of the relaxation parameter for when Nx=Ny=10. Reference Appendix 5.1 for python script. 
\begin{lstlisting}
Convergence reached after 73 iterations with omega=1.0.
Convergence reached after 67 iterations with omega=1.05.
Convergence reached after 61 iterations with omega=1.1.
Convergence reached after 56 iterations with omega=1.1500000000000001.
Convergence reached after 51 iterations with omega=1.2000000000000002.
Convergence reached after 47 iterations with omega=1.2500000000000002.
Convergence reached after 42 iterations with omega=1.3000000000000003.
Convergence reached after 38 iterations with omega=1.3500000000000003.
Convergence reached after 34 iterations with omega=1.4000000000000004.
Convergence reached after 29 iterations with omega=1.4500000000000004.
Convergence reached after 25 iterations with omega=1.5000000000000004.
Convergence reached after 22 iterations with omega=1.5500000000000005.
Convergence reached after 23 iterations with omega=1.6000000000000005.
Convergence reached after 26 iterations with omega=1.6500000000000006.
Convergence reached after 29 iterations with omega=1.7000000000000006.
Convergence reached after 41 iterations with omega=1.7500000000000007.
Convergence reached after 44 iterations with omega=1.8000000000000007.
Convergence reached after 62 iterations with omega=1.8500000000000008.
Convergence reached after 92 iterations with omega=1.9000000000000008.
Convergence reached after 187 iterations with omega=1.9500000000000008.
\end{lstlisting}

\textbf{Optimal relaxation parameter for Nx=Ny=10: $\omega_o$ = 1.55, requiring 22 iterations.}

\newpage 
\section{Use the point Gauss-Seidel SOR method to find the optimal relaxation parameter value $\omega _o$ =1.80 for Nx = Ny = 50}
This question was tasked with varying the relaxation parameter $\omega$ between 1.0 $\leq \omega \leq $ 1.95 with a step size of 0.05. So you will have 20 different values of relaxation parameter $\omega$, with $\omega$ = 1.0, 1.05, 1.1, 1.15, ..., 1.95. For each value of relaxation parameter $\omega$, find the number of iterations required for convergence $k_0$ according to the convergence criterion we set in Eqn. 5. Lastly, the number of iterations required for convergence, $k_0$, depends on $\omega$. $k_0 = k_0(\omega)$. In Appendix section 5.1, reference lines 91-119 in my python script for clarity on how the following plot and optimal value of the relaxation parameter were determined.

\subsection{x-y plot of the “number of iterations required for convergence $k_o vs. \omega$}
When the grid size is larger, there are more interactions among grid points, making the convergence process more complex. With Nx=Ny=50, the optimal value of $\omega$ might be more challenging to determine due to the increased number of neighboring points affecting each other. The larger grid typically requires more iterations for convergence, which could lead to a linear pattern in the plot, with a wider range of $\omega$ values yielding acceptable convergence. Lastly, I set the max iterations in my code to be 1,000. If that limit was set higher, this plot would form a more uniform U-shape. You can see at around $\omega$ =1.9, the plot starts to go upwards again, if the iterations limit was higher, that line would continue on a similar trajectory and create a U-shape plot. 
\begin{center}
    \includegraphics[width=15.00cm]{output_0_10.png} \\
    \textit{Figure 8 - number of iterations required for convergence $k_o vs. \omega$ when Nx=Ny=50 }
\end{center}

\newpage
\subsection{For Nx = Ny = 50, report the optimal value of the relaxation parameter, $\omega_o$}
Below is the optimal value of the relaxation parameter for when Nx=Ny=50. Reference Appendix 5.1 for python script. 
\begin{lstlisting}
Convergence reached after 986 iterations with omega=1.0.
Convergence reached after 916 iterations with omega=1.05.
Convergence reached after 850 iterations with omega=1.1.
Convergence reached after 787 iterations with omega=1.1500000000000001.
Convergence reached after 728 iterations with omega=1.2000000000000002.
Convergence reached after 672 iterations with omega=1.2500000000000002.
Convergence reached after 618 iterations with omega=1.3000000000000003.
Convergence reached after 567 iterations with omega=1.3500000000000003.
Convergence reached after 518 iterations with omega=1.4000000000000004.
Convergence reached after 470 iterations with omega=1.4500000000000004.
Convergence reached after 425 iterations with omega=1.5000000000000004.
Convergence reached after 380 iterations with omega=1.5500000000000005.
Convergence reached after 337 iterations with omega=1.6000000000000005.
Convergence reached after 295 iterations with omega=1.6500000000000006.
Convergence reached after 254 iterations with omega=1.7000000000000006.
Convergence reached after 213 iterations with omega=1.7500000000000007.
Convergence reached after 171 iterations with omega=1.8000000000000007.
Convergence reached after 128 iterations with omega=1.8500000000000008.
Convergence reached after 104 iterations with omega=1.9000000000000008.
Convergence reached after 201 iterations with omega=1.9500000000000008.
\end{lstlisting}
\textbf{Optimal relaxation parameter for Nx=Ny=50: $\omega_o$ = 1.90, requiring 104 iterations.}


\newpage 
\section{Appendix}

\subsection{Full python script}
\begin{lstlisting}
import numpy as np
import matplotlib.pyplot as plt


# Function to solve 2D Laplace's equation using Gauss-Seidel SOR
def laplace_2d_sor(Nx, Ny, max_iter=10000, tol=1e-4, omega=1.8):
    phi = np.zeros((Nx + 1, Ny + 1))
    
    # Boundary conditions
    x = np.linspace(0, 1, Nx + 1)
    y = np.linspace(0, 1, Ny + 1)
    phi[:, 0] = x  # Bottom boundary
    phi[0, 1:] = y[1:]  # Left boundary
    phi[-1, 1:] = 1  # Right boundary
    phi[1:, -1] = 1  # Top boundary
    
    convergence_history = []
    converged = False
    iteration = 0
    
    while not converged and iteration < max_iter:
        iteration += 1
        max_change = 0
        for i in range(1, Nx):
            for j in range(1, Ny):
                old_phi = phi[i, j]
                phi[i, j] = (1 - omega) * old_phi + omega * 0.25 * (
                    phi[i + 1, j] + phi[i - 1, j] + phi[i, j + 1] + phi[i, j - 1]
                )
                change = abs(phi[i, j] - old_phi)
                if change > max_change:
                    max_change = change
        
        relative_change = max_change / np.max(np.abs(phi))
        convergence_history.append(relative_change)
        
        if relative_change < tol:
            converged = True
            print(f"Convergence reached after {iteration} iterations with omega={omega}.")
    
    return phi, convergence_history, iteration

# Function to plot the numerical solution at y = 0.5
def plot_numerical_solution_y0_5(phi, Ny):
    plt.figure(figsize=(8, 6))
    x = np.linspace(0, 1, Ny + 1)
    temperature_y0_5 = phi[:, int(Ny / 2)]
    plt.plot(x, temperature_y0_5, '-o', label='Numerical Solution')
    plt.xlabel('x coordinate ($x_i$)')
    plt.ylabel('Numerical value ($T_{i,j}$)')
    plt.title('Numerical Solution at y=0.5')
    plt.grid(True)
    plt.show()

# Given parameters
N_values = [(10, 10), (50, 50)]
omega = 1.8

# Iterate over each (Nx, Ny) pair
for Nx, Ny in N_values:
    # Solve the Laplace equation using SOR
    phi, convergence_history, num_iterations = laplace_2d_sor(Nx, Ny, omega=omega)

    # Contour plot of the final converged numerical solution
    plt.figure(figsize=(8, 6))
    X, Y = np.meshgrid(np.linspace(0, 1, Nx + 1), np.linspace(0, 1, Ny + 1))
    plt.contourf(X, Y, phi.T, levels=20, cmap='hot')
    plt.colorbar(label='Potential ($\\phi$)')
    plt.title(f'2D Laplace Equation Solution (Contour Plot) - Nx={Nx}, Ny={Ny}')
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.show()

    # Plot of the numerical solution at y = 0.5
    plot_numerical_solution_y0_5(phi, Ny)

    # Plot of the convergence history
    plt.figure(figsize=(8, 6))
    plt.plot(convergence_history, '-o')
    plt.title('Convergence History')
    plt.xlabel('Iteration index ($k$)')
    plt.ylabel('$\delta(k) / \delta(0)$')
    plt.yscale('log')
    plt.grid(True)
    plt.show()

    # Additional step: finding the last value in the convergence history
    last_value = convergence_history[-1]
    print(f"For Nx={Nx}, Ny={Ny}, the last value in the convergence history is: {last_value}")

# Iterate over relaxation parameter ω from 1.0 to 1.95 in increments of 0.05
omega_values = np.arange(1.0, 2.0, 0.05)
iterations_needed = {}

for Nx in [10, 50]:
    k0_omega = []
    for omega in omega_values:
        _, _, num_iterations = laplace_2d_sor(Nx, Nx, omega=omega)
        k0_omega.append(num_iterations)
    
    iterations_needed[Nx] = k0_omega

# Plot the relationship between ω and the number of iterations required for convergence
for Nx in [10, 50]:
    plt.figure(figsize=(8, 6))
    plt.plot(omega_values, iterations_needed[Nx], '-o', label=f'Nx={Nx}, Ny={Nx}')
    plt.xlabel('Relaxation parameter ($\\omega$)')
    plt.ylabel('Number of iterations ($k_0$)')
    plt.title(f'Convergence vs Relaxation Parameter (Nx=Ny={Nx})')
    plt.grid(True)
    plt.show()

# Find the optimal relaxation parameter ω_o for each iteration value
optimal_omega = {}
for Nx in [10, 50]:
    min_iterations = min(iterations_needed[Nx])
    optimal_omega_value = omega_values[iterations_needed[Nx].index(min_iterations)]
    optimal_omega[Nx] = optimal_omega_value
    print(f"Optimal relaxation parameter for Nx=Ny={Nx}: ω_o = {optimal_omega_value:.2f}, requiring {min_iterations} iterations.")


\end{lstlisting}
\subsection{Convergence for when $\omega$ = 1.80, Nx=Ny=10}
\begin{lstlisting}
    Convergence reached after 44 iterations with omega=1.80 and Nx=Ny=10.
\end{lstlisting}

\subsection{Convergence history for when $\omega$ = 1.80, Nx=Ny=10}
\begin{lstlisting}
For Nx=10, Ny=10, the last value in the convergence history is:
8.534642242818968e-05
\end{lstlisting}

\subsection{Convergence for when $\omega$ = 1.80, Nx=Ny=50}
\begin{lstlisting}
    Convergence reached after 171 iterations with omega=1.80 and Nx=Ny=50.
\end{lstlisting}

\subsection{Convergence history for when $\omega$ = 1.80, Nx=Ny=50}
\begin{lstlisting}
For Nx=50, Ny=50, the last value in the convergence history is:
9.954540001710921e-05
\end{lstlisting}

\subsection{Convergence vs. Relaxation Parameter for Nx=Ny=10}
\begin{lstlisting}
Convergence reached after 73 iterations with omega=1.0.
Convergence reached after 67 iterations with omega=1.05.
Convergence reached after 61 iterations with omega=1.1.
Convergence reached after 56 iterations with omega=1.1500000000000001.
Convergence reached after 51 iterations with omega=1.2000000000000002.
Convergence reached after 47 iterations with omega=1.2500000000000002.
Convergence reached after 42 iterations with omega=1.3000000000000003.
Convergence reached after 38 iterations with omega=1.3500000000000003.
Convergence reached after 34 iterations with omega=1.4000000000000004.
Convergence reached after 29 iterations with omega=1.4500000000000004.
Convergence reached after 25 iterations with omega=1.5000000000000004.
Convergence reached after 22 iterations with omega=1.5500000000000005.
Convergence reached after 23 iterations with omega=1.6000000000000005.
Convergence reached after 26 iterations with omega=1.6500000000000006.
Convergence reached after 29 iterations with omega=1.7000000000000006.
Convergence reached after 41 iterations with omega=1.7500000000000007.
Convergence reached after 44 iterations with omega=1.8000000000000007.
Convergence reached after 62 iterations with omega=1.8500000000000008.
Convergence reached after 92 iterations with omega=1.9000000000000008.
Convergence reached after 187 iterations with omega=1.9500000000000008.
\end{lstlisting}

\textbf{Optimal relaxation parameter for Nx=Ny=10: $\omega_o$ = 1.55, requiring 22 iterations.}

\subsection{Convergence vs. Relaxation Parameter for Nx=Ny=50}
\begin{lstlisting}
Convergence reached after 986 iterations with omega=1.0.
Convergence reached after 916 iterations with omega=1.05.
Convergence reached after 850 iterations with omega=1.1.
Convergence reached after 787 iterations with omega=1.1500000000000001.
Convergence reached after 728 iterations with omega=1.2000000000000002.
Convergence reached after 672 iterations with omega=1.2500000000000002.
Convergence reached after 618 iterations with omega=1.3000000000000003.
Convergence reached after 567 iterations with omega=1.3500000000000003.
Convergence reached after 518 iterations with omega=1.4000000000000004.
Convergence reached after 470 iterations with omega=1.4500000000000004.
Convergence reached after 425 iterations with omega=1.5000000000000004.
Convergence reached after 380 iterations with omega=1.5500000000000005.
Convergence reached after 337 iterations with omega=1.6000000000000005.
Convergence reached after 295 iterations with omega=1.6500000000000006.
Convergence reached after 254 iterations with omega=1.7000000000000006.
Convergence reached after 213 iterations with omega=1.7500000000000007.
Convergence reached after 171 iterations with omega=1.8000000000000007.
Convergence reached after 128 iterations with omega=1.8500000000000008.
Convergence reached after 104 iterations with omega=1.9000000000000008.
Convergence reached after 201 iterations with omega=1.9500000000000008.
\end{lstlisting}
\textbf{Optimal relaxation parameter for Nx=Ny=50: $\omega_o$ = 1.90, requiring 104 iterations.}

\end{document}
